[
    {
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "latency_test_prompts": 10,
            "throughput_test_prompts": 1000,
            "concurrency": 100,
            "max_input_length": 256,
            "max_output_length": 256,
            "request_timeout": 180
        },
        "results": {
            "p1_ttft_ms": 21.36,
            "p50_ttft_ms": 21.65,
            "p95_ttft_ms": 51.85,
            "p99_ttft_ms": 71.28,
            "p1_tbst_ms": 16.38,
            "p50_tbst_ms": 16.77,
            "p95_tbst_ms": 19.69,
            "p99_tbst_ms": 21.35,
            "p1_mean_itl_ms": 16.73,
            "p50_mean_itl_ms": 17.08,
            "p95_mean_itl_ms": 17.09,
            "p99_mean_itl_ms": 17.1,
            "total_duration_sec": 69.45,
            "successful_requests": 1000,
            "failed_requests": 0,
            "total_tokens_generated": 236574,
            "throughput_req_per_sec": 14.4,
            "throughput_tokens_per_sec": 3406.34,
            "errors": []
        },
        "run_name": "Llama-3-3-8b - 256 / 256",
        "client_log_file": "1_Llama-3-3-8b_-_256__256_client.log",
        "status": "SUCCESS"
    },
    {
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "latency_test_prompts": 10,
            "throughput_test_prompts": 1000,
            "concurrency": 100,
            "max_input_length": 1000,
            "max_output_length": 1000,
            "request_timeout": 180
        },
        "results": {
            "p1_ttft_ms": 27.44,
            "p50_ttft_ms": 27.81,
            "p95_ttft_ms": 29.86,
            "p99_ttft_ms": 31.12,
            "p1_tbst_ms": 17.19,
            "p50_tbst_ms": 17.31,
            "p95_tbst_ms": 17.74,
            "p99_tbst_ms": 17.85,
            "p1_mean_itl_ms": 17.33,
            "p50_mean_itl_ms": 17.34,
            "p95_mean_itl_ms": 17.34,
            "p99_mean_itl_ms": 17.34,
            "total_duration_sec": 292.52,
            "successful_requests": 1000,
            "failed_requests": 0,
            "total_tokens_generated": 1000000,
            "throughput_req_per_sec": 3.42,
            "throughput_tokens_per_sec": 3418.59,
            "errors": []
        },
        "run_name": "Llama-3-3-8b - 1000 / 1000",
        "client_log_file": "2_Llama-3-3-8b_-_1000__1000_client.log",
        "status": "SUCCESS"
    }
]