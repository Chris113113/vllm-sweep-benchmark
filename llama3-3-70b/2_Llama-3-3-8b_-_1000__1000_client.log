--- Client Run: Llama-3-3-8b - 1000 / 1000 ---

--- Client STDOUT ---
{
  "config": {
    "model": "meta-llama/Llama-3.3-70B-Instruct",
    "num_prompts": 1000,
    "concurrency": 125,
    "max_input_length": 1000,
    "max_output_length": 1000,
    "request_timeout_sec": 180
  },
  "results": {
    "total_duration_sec": 291.69,
    "successful_requests": 975,
    "failed_requests": 25,
    "total_tokens_generated": 962810,
    "throughput_req_per_sec": 3.34,
    "throughput_tokens_per_sec": 3300.78,
    "avg_inter_token_latency_ms": 0.3,
    "errors": [
      "Status: 408, Body: Request timed out after 180 seconds",
      "Status: 408, Body: Request timed out after 180 seconds",
      "Status: 408, Body: Request timed out after 180 seconds",
      "Status: 408, Body: Request timed out after 180 seconds",
      "Status: 408, Body: Request timed out after 180 seconds"
    ]
  }
}

--- Client STDERR ---
Client: Starting generation-focused benchmark with a constant concurrency of 125...
Client: Generating 1000 synthetic prompts of length 1000...
Client: Completed 100/1000 requests...
Client: Completed 200/1000 requests...
Client: Completed 300/1000 requests...
Client: Completed 400/1000 requests...
Client: Completed 500/1000 requests...
Client: Completed 600/1000 requests...
Client: Completed 700/1000 requests...
Client: Completed 800/1000 requests...
Client: Completed 900/1000 requests...
Client: Completed 1000/1000 requests...

================================================================================
Client Benchmark Summary
================================================================================
Total Time Taken: 291.69 seconds
Total Requests Sent: 1000
Successful Requests: 975
Failed Requests: 25

--- Performance ---
Throughput (Requests/sec): 3.34
Throughput (Tokens/sec): 3300.78
Average Inter-Token Latency: 0.30 ms
================================================================================
