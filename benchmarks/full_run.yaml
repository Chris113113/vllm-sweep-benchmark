# ==============================================================================
# VLLM Automated Benchmark Configuration
# ==============================================================================
#
# This file defines a suite of standalone benchmark runs using the official
# vLLM benchmark script (benchmarks/benchmark_throughput.py).
#
# The orchestrator will generate a test for each combination of
# `tensor_parallel_sizes` and `length_configs`.

standalone_runs:
  - name: "Llama-3.3-70B-Instruct"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [2, 4, 8]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "meta-llama/Llama-3.3-70B-Instruct"
      --num-prompts 200
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Llama-3.1-8B-Instruct"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "meta-llama/Llama-3.1-8B-Instruct"
      --num-prompts 500
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Gemma-3-27B-IT"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2, 4]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "google/gemma-3-27b-it"
      --num-prompts 200
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Qwen3-32B"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2, 4, 8]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "Qwen/Qwen3-32B"
      --num-prompts 200
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Qwen3-4B"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "Qwen/Qwen3-4B"
      --num-prompts 500
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Llama-4-Maverick-17B-128E-Instruct"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2, 4, 8]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1024, output_len: 1024 }
      - { input_len: 4096, output_len: 4096 }
    args: >
      --model "meta-llama/Llama-4-Maverick-17B-128E-Instruct"
      --num-prompts 200
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "DeepSeek-R1-671B"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [2, 4, 8]
    length_configs:
      - { input_len: 256, output_len: 256 }
      - { input_len: 1000, output_len: 1000 }
    args: >
      --model "deepseek-ai/DeepSeek-R1"
      --num-prompts 50
      --gpu-memory-utilization 0.9
      --quantization fp8

  - name: "Llama-Guard-4-12B"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1, 2]
    length_configs:
      - { input_len: 256, output_len: 64 }
      - { input_len: 1024, output_len: 64 }
      - { input_len: 5000, output_len: 64 }
    args: >
      --model "meta-llama/Llama-4-Guard-12B"
      --num-prompts 500
      --gpu-memory-utilization 0.9
