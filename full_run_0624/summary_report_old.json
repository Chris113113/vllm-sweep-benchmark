[
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-2_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 21.35,
            "throughput_total_tokens_per_sec": 10930.62,
            "throughput_output_tokens_per_sec": 5465.52,
            "total_prompt_tokens": 51196,
            "total_output_tokens": 51200
        },
        "client_log_file": "1_Llama-3.3-70B-Instruct_TP-2_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-4_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 26.3,
            "throughput_total_tokens_per_sec": 13463.58,
            "throughput_output_tokens_per_sec": 6731.79,
            "total_prompt_tokens": 51200,
            "total_output_tokens": 51200
        },
        "client_log_file": "2_Llama-3.3-70B-Instruct_TP-4_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-8_ISL-256_OSL-256",
        "status": "TERMINATED_DUE_TO_ERROR"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-2_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {
            "throughput_req_per_sec": 4.28,
            "throughput_total_tokens_per_sec": 8765.34,
            "throughput_output_tokens_per_sec": 4384.46,
            "total_prompt_tokens": 204633,
            "total_output_tokens": 204800
        },
        "client_log_file": "4_Llama-3.3-70B-Instruct_TP-2_ISL-1024_OSL-1024_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-4_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {
            "throughput_req_per_sec": 5.64,
            "throughput_total_tokens_per_sec": 11554.73,
            "throughput_output_tokens_per_sec": 5777.37,
            "total_prompt_tokens": 204800,
            "total_output_tokens": 204800
        },
        "client_log_file": "5_Llama-3.3-70B-Instruct_TP-4_ISL-1024_OSL-1024_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-8_ISL-1024_OSL-1024",
        "status": "TERMINATED_DUE_TO_ERROR"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-2_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {
            "throughput_req_per_sec": 0.43,
            "throughput_total_tokens_per_sec": 3484.81,
            "throughput_output_tokens_per_sec": 1742.94,
            "total_prompt_tokens": 818695,
            "total_output_tokens": 819200
        },
        "client_log_file": "7_Llama-3.3-70B-Instruct_TP-2_ISL-4096_OSL-4096_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-4_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {
            "throughput_req_per_sec": 0.85,
            "throughput_total_tokens_per_sec": 6983.13,
            "throughput_output_tokens_per_sec": 3491.7,
            "total_prompt_tokens": 819138,
            "total_output_tokens": 819200
        },
        "client_log_file": "8_Llama-3.3-70B-Instruct_TP-4_ISL-4096_OSL-4096_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.3-70B-Instruct_TP-8_ISL-4096_OSL-4096",
        "status": "TERMINATED_DUE_TO_ERROR"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {},
        "client_log_file": "10_Llama-3.1-8B-Instruct_TP-1_ISL-256_OSL-256_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {},
        "client_log_file": "11_Llama-3.1-8B-Instruct_TP-2_ISL-256_OSL-256_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "12_Llama-3.1-8B-Instruct_TP-1_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "13_Llama-3.1-8B-Instruct_TP-2_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "14_Llama-3.1-8B-Instruct_TP-1_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "15_Llama-3.1-8B-Instruct_TP-2_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {},
        "client_log_file": "16_Gemma-3-27B-IT_TP-1_ISL-256_OSL-256_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {},
        "client_log_file": "17_Gemma-3-27B-IT_TP-2_ISL-256_OSL-256_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {},
        "client_log_file": "18_Gemma-3-27B-IT_TP-4_ISL-256_OSL-256_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "19_Gemma-3-27B-IT_TP-1_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "20_Gemma-3-27B-IT_TP-2_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "21_Gemma-3-27B-IT_TP-4_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "22_Gemma-3-27B-IT_TP-1_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "23_Gemma-3-27B-IT_TP-2_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "24_Gemma-3-27B-IT_TP-4_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-8_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-2_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-4_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-8_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-256_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-256_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-1024_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-1024_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-5000_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-5000_OSL-64",
        "status": "NOT_STARTED"
    }
]