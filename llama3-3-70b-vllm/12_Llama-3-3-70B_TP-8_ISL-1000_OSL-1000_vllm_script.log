--- STDOUT ---
INFO 06-24 23:55:59 [__init__.py:244] Automatically detected platform cuda.
When dataset path is not set, it will default to random dataset
Namespace(backend='vllm', dataset_name='random', dataset=None, dataset_path=None, input_len=1000, output_len=1000, n=1, num_prompts=100, hf_max_batch_size=None, output_json=None, async_engine=False, disable_frontend_multiprocessing=False, disable_detokenize=False, lora_path=None, prefix_len=None, random_range_ratio=None, hf_subset=None, hf_split=None, model='meta-llama/Llama-3.3-70B-Instruct', task='auto', tokenizer='meta-llama/Llama-3.3-70B-Instruct', tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2000, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=8, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, data_parallel_backend='mp', enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', enable_multimodal_encoder_data_parallel=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', disable_hybrid_kv_cache_manager=False, kv_transfer_config=None, kv_events_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":null,"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":null,"local_cache_dir":null}, additional_config={}, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False)
INFO 06-24 23:56:07 [config.py:823] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 06-24 23:56:07 [config.py:1946] Defaulting to use mp for distributed inference
INFO 06-24 23:56:07 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-24 23:56:08 [core.py:455] Waiting for init message from front-end.
INFO 06-24 23:56:08 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 06-24 23:56:08 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_b339eabd'), local_subscribe_addr='ipc:///tmp/d88dc49a-7d5d-41e0-9fac-8a976606f56b', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c4ace896a0>
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8982cad4'), local_subscribe_addr='ipc:///tmp/ea832eaa-3ff1-4893-b792-d1245966c90a', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5afc0>
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f2ea9a49'), local_subscribe_addr='ipc:///tmp/b0e9b91e-01fe-4ab6-86d6-e506d6b8acb8', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5aa20>
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_57a0d50e'), local_subscribe_addr='ipc:///tmp/d11bf55b-9eef-40ca-a9d0-0f8fed41354e', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5ad50>
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c4ace896d0>
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_90ccff1a'), local_subscribe_addr='ipc:///tmp/7cbf054a-7ddb-4c06-be93-40ad7255d99e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0f179d34'), local_subscribe_addr='ipc:///tmp/a62e82f3-32f3-427e-a913-84b34e94c5d9', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5a4e0>
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4f22867f'), local_subscribe_addr='ipc:///tmp/96fde780-9ab4-447c-a2b4-06a5ce6c8da6', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5a4b0>
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bee3de6a'), local_subscribe_addr='ipc:///tmp/fe3f7b97-be63-4c7d-9b97-e54217e508a1', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-24 23:56:08 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c0aea5a0f0>
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_222406c3'), local_subscribe_addr='ipc:///tmp/eed8240d-914a-4ec7-8316-866887efcc4f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:19 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:19 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/pirillo_google_com/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_f7791135'), local_subscribe_addr='ipc:///tmp/a3ed251e-5260-4df4-9a9a-0d6e96df4ade', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(VllmWorker rank=0 pid=22350)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [parallel_state.py:1065] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=3 pid=22353)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m WARNING 06-24 23:56:52 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [cuda.py:246] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:52 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:56:53 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:00 [default_loader.py:272] Loading weights took 6.58 seconds
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:00 [default_loader.py:272] Loading weights took 6.90 seconds
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:00 [default_loader.py:272] Loading weights took 6.81 seconds
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:00 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 7.543822 seconds
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:00 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 7.655387 seconds
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:00 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 7.636450 seconds
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:00 [default_loader.py:272] Loading weights took 6.43 seconds
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:01 [default_loader.py:272] Loading weights took 6.89 seconds
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:01 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 8.196838 seconds
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:01 [default_loader.py:272] Loading weights took 7.10 seconds
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:01 [default_loader.py:272] Loading weights took 6.84 seconds
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:01 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 8.459999 seconds
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:01 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 8.618598 seconds
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:01 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 8.710260 seconds
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:02 [default_loader.py:272] Loading weights took 8.50 seconds
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:02 [gpu_model_runner.py:1624] Model loading took 16.4607 GiB and 9.564288 seconds
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.94 s
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_4_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_5_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.84 s
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.93 s
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.91 s
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_6_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.93 s
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_7_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.98 s
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 11.00 s
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:13 [backends.py:462] Using cache directory: /home/pirillo_google_com/.cache/vllm/torch_compile_cache/2769c6bbaa/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:13 [backends.py:472] Dynamo bytecode transform time: 10.99 s
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:16 [backends.py:161] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.84 s
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.84 s
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.87 s
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.96 s
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.82 s
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.95 s
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.87 s
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:57:52 [backends.py:173] Compiling a graph for general shape takes 37.89 s
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.89 s in total
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.77 s in total
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.68 s in total
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.88 s in total
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.87 s in total
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.80 s in total
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.86 s in total
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:58:15 [monitor.py:34] torch.compile takes 48.77 s in total
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 136.45 GiB
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.95 GiB
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:58:17 [gpu_worker.py:227] Available KV cache memory: 135.82 GiB
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,563,712 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1781.86x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,560,432 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1780.22x
INFO 06-24 23:58:18 [kv_cache_utils.py:715] GPU KV cache size: 3,576,816 tokens
INFO 06-24 23:58:18 [kv_cache_utils.py:719] Maximum concurrency for 2,000 tokens per request: 1788.41x
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:58:49 [custom_all_reduce.py:196] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=7 pid=22357)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=5 pid=22355)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=4 pid=22354)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=3 pid=22353)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=2 pid=22352)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=1 pid=22351)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=0 pid=22350)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
[1;36m(VllmWorker rank=6 pid=22356)[0;0m INFO 06-24 23:58:50 [gpu_model_runner.py:2048] Graph capturing finished in 32 secs, took 1.25 GiB
INFO 06-24 23:58:50 [core.py:171] init engine (profile, create kv cache, warmup model) took 107.79 seconds
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=5 pid=22355)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=4 pid=22354)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=6 pid=22356)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=7 pid=22357)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=0 pid=22350)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=3 pid=22353)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=1 pid=22351)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
ERROR 06-24 23:58:53 [dump_input.py:69] Dumping input data
ERROR 06-24 23:58:53 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}, 
ERROR 06-24 23:58:53 [dump_input.py:79] Dumping scheduler output for model execution:
ERROR 06-24 23:58:53 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=65,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=66,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=67,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=68,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=69,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=70,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=71,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=72,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=73,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=74,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=75,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=76,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=77,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=78,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=79,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027],),num_computed_tokens=0,lora_request=None), NewRequestData(req_id=80,prompt_token_ids_len=1000,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059],),num_computed_tokens=0,lora_request=None)], scheduled_cached_reqs=[CachedRequestData(req_id='0', resumed_from_preemption=false, new_token_ids=[10088], new_block_ids=[[]], num_computed_tokens=1004), CachedRequestData(req_id='1', resumed_from_preemption=false, new_token_ids=[23067], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='2', resumed_from_preemption=false, new_token_ids=[29968], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='3', resumed_from_preemption=false, new_token_ids=[30837], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='4', resumed_from_preemption=false, new_token_ids=[37478], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='5', resumed_from_preemption=false, new_token_ids=[29214], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='6', resumed_from_preemption=false, new_token_ids=[126167], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='7', resumed_from_preemption=false, new_token_ids=[6891], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='8', resumed_from_preemption=false, new_token_ids=[16606], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='9', resumed_from_preemption=false, new_token_ids=[5628], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='10', resumed_from_preemption=false, new_token_ids=[14584], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='11', resumed_from_preemption=false, new_token_ids=[46889], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='12', resumed_from_preemption=false, new_token_ids=[4723], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='13', resumed_from_preemption=false, new_token_ids=[348], new_block_ids=[[]], num_computed_tokens=924), CachedRequestData(req_id='14', resumed_from_preemption=false, new_token_ids=[264], new_block_ids=[[]], num_computed_tokens=1003), CachedRequestData(req_id='15', resumed_from_preemption=false, new_token_ids=[7069], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='16', resumed_from_preemption=false, new_token_ids=[47], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='17', resumed_from_preemption=false, new_token_ids=[55375], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='18', resumed_from_preemption=false, new_token_ids=[8445], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='19', resumed_from_preemption=false, new_token_ids=[68875], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='20', resumed_from_preemption=false, new_token_ids=[4880], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='21', resumed_from_preemption=false, new_token_ids=[32645], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='22', resumed_from_preemption=false, new_token_ids=[100819], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='23', resumed_from_preemption=false, new_token_ids=[29097], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='24', resumed_from_preemption=false, new_token_ids=[104615], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='25', resumed_from_preemption=false, new_token_ids=[128003], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='26', resumed_from_preemption=false, new_token_ids=[239], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='27', resumed_from_preemption=false, new_token_ids=[3597], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='28', resumed_from_preemption=false, new_token_ids=[87212], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='29', resumed_from_preemption=false, new_token_ids=[5527], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='30', resumed_from_preemption=false, new_token_ids=[81386], new_block_ids=[[]], num_computed_tokens=1002), CachedRequestData(req_id='31', resumed_from_preemption=false, new_token_ids=[13879], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='32', resumed_from_preemption=false, new_token_ids=[112756], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='33', resumed_from_preemption=false, new_token_ids=[29144], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='34', resumed_from_preemption=false, new_token_ids=[93], new_block_ids=[[]], num_computed_tokens=875), CachedRequestData(req_id='35', resumed_from_preemption=false, new_token_ids=[72252], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='36', resumed_from_preemption=false, new_token_ids=[14765], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='37', resumed_from_preemption=false, new_token_ids=[85586], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='38', resumed_from_preemption=false, new_token_ids=[6529], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='39', resumed_from_preemption=false, new_token_ids=[331], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='40', resumed_from_preemption=false, new_token_ids=[6089], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='41', resumed_from_preemption=false, new_token_ids=[30239], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='42', resumed_from_preemption=false, new_token_ids=[69246], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='43', resumed_from_preemption=false, new_token_ids=[84585], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='44', resumed_from_preemption=false, new_token_ids=[73064], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='45', resumed_from_preemption=false, new_token_ids=[5475], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='46', resumed_from_preemption=false, new_token_ids=[24030], new_block_ids=[[]], num_computed_tokens=1001), CachedRequestData(req_id='47', resumed_from_preemption=false, new_token_ids=[36026], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='48', resumed_from_preemption=false, new_token_ids=[32674], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='49', resumed_from_preemption=false, new_token_ids=[121002], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='50', resumed_from_preemption=false, new_token_ids=[36171], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='51', resumed_from_preemption=false, new_token_ids=[18205], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='52', resumed_from_preemption=false, new_token_ids=[36095], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='53', resumed_from_preemption=false, new_token_ids=[126813], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='54', resumed_from_preemption=false, new_token_ids=[52072], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='55', resumed_from_preemption=false, new_token_ids=[16891], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='56', resumed_from_preemption=false, new_token_ids=[49715], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='57', resumed_from_preemption=false, new_token_ids=[93396], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='58', resumed_from_preemption=false, new_token_ids=[49540], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='59', resumed_from_preemption=false, new_token_ids=[22300], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='60', resumed_from_preemption=false, new_token_ids=[89954], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='61', resumed_from_preemption=false, new_token_ids=[635], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='62', resumed_from_preemption=false, new_token_ids=[28680], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='63', resumed_from_preemption=false, new_token_ids=[35578], new_block_ids=[[]], num_computed_tokens=1000), CachedRequestData(req_id='64', resumed_from_preemption=false, new_token_ids=[8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 278, 532, 273, 75, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 831, 69625, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 2219, 700, 16158, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 5809, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 7, 2470, 8229, 8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 5809, 8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259, 8260, 459, 31177, 1849, 8263, 8264, 8265, 8266, 8267, 8268, 8269, 8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 13, 4806, 8298, 26857, 12261, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315, 8316, 7, 372, 556, 2050, 8320, 5809, 8322, 8323, 8324, 1883, 1978, 258, 8327, 8328, 8329, 8330, 8331, 8332, 8333, 8334, 8335, 20897, 1140, 2240, 8338, 8339, 8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349, 8350, 8351, 8352, 8353, 62, 680, 8355, 8356, 8357, 8358, 8359, 8360, 8361, 8362, 8363, 8364, 17780, 332, 478, 1091, 82061, 45, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 65, 1965, 376, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399, 8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409, 8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419, 8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429, 8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 1687, 22635, 1003, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499, 8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509, 8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 82601, 10216, 1013, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529, 8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539, 8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549, 8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 663, 5850, 65, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 21364, 365, 79, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8595, 1542, 1557, 301, 8598, 8599, 8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609, 8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619, 8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629, 8630, 8631, 8632, 8633, 62, 2196, 8635, 8636, 8637, 8638, 8639, 8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669, 16686, 939, 88, 8672, 8673, 8674, 8675, 5809, 8677, 8678, 8679, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 5809, 8688, 8689, 8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719, 8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739, 8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759, 8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 13, 5038, 258, 8779, 8780, 8781, 8782, 18631, 3380, 8785, 8786, 8787, 8788, 8789, 30433, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799, 14848, 316, 437, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809, 8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839, 8840, 8841, 8842, 8843, 29658, 26172, 8846, 8847, 8848, 8849, 8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859, 3762, 8618, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869, 8870, 8871], new_block_ids=[[4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082]], num_computed_tokens=185)], num_scheduled_tokens={78: 1000, 50: 1, 4: 1, 41: 1, 2: 1, 14: 1, 10: 1, 42: 1, 56: 1, 52: 1, 40: 1, 57: 1, 67: 1000, 35: 1, 11: 1, 51: 1, 3: 1, 61: 1, 26: 1, 20: 1, 80: 505, 46: 1, 72: 1000, 58: 1, 24: 1, 45: 1, 23: 1, 28: 1, 76: 1000, 77: 1000, 12: 1, 17: 1, 21: 1, 63: 1, 64: 815, 0: 1, 70: 1000, 1: 1, 60: 1, 65: 1000, 44: 1, 29: 1, 74: 1000, 47: 1, 55: 1, 54: 1, 7: 1, 39: 1, 8: 1, 43: 1, 33: 1, 18: 1, 66: 1000, 22: 1, 49: 1, 13: 1, 62: 1, 75: 1000, 68: 1000, 34: 1, 79: 1000, 59: 1, 32: 1, 5: 1, 73: 1000, 53: 1, 15: 1, 27: 1, 25: 1, 19: 1, 36: 1, 31: 1, 38: 1, 6: 1, 37: 1, 69: 1000, 71: 1000, 9: 1, 30: 1, 16: 1, 48: 1}, total_num_scheduled_tokens=16384, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
ERROR 06-24 23:58:53 [dump_input.py:82] SchedulerStats(num_running_reqs=81, num_waiting_reqs=19, gpu_cache_usage=0.02273881371698716, prefix_cache_stats=PrefixCacheStats(reset=False, requests=16, queries=16000, hits=0), spec_decoding_stats=None)
ERROR 06-24 23:58:53 [core.py:517] EngineCore encountered a fatal error.
ERROR 06-24 23:58:53 [core.py:517] Traceback (most recent call last):
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 508, in run_engine_core
ERROR 06-24 23:58:53 [core.py:517]     engine_core.run_busy_loop()
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 535, in run_busy_loop
ERROR 06-24 23:58:53 [core.py:517]     self._process_engine_step()
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 560, in _process_engine_step
ERROR 06-24 23:58:53 [core.py:517]     outputs, model_executed = self.step_fn()
ERROR 06-24 23:58:53 [core.py:517]                               ^^^^^^^^^^^^^^
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in step
ERROR 06-24 23:58:53 [core.py:517]     model_output = self.execute_model(scheduler_output)
ERROR 06-24 23:58:53 [core.py:517]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 217, in execute_model
ERROR 06-24 23:58:53 [core.py:517]     raise err
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 211, in execute_model
ERROR 06-24 23:58:53 [core.py:517]     return self.model_executor.execute_model(scheduler_output)
ERROR 06-24 23:58:53 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 163, in execute_model
ERROR 06-24 23:58:53 [core.py:517]     (output, ) = self.collective_rpc("execute_model",
ERROR 06-24 23:58:53 [core.py:517]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 220, in collective_rpc
ERROR 06-24 23:58:53 [core.py:517]     result = get_response(w, dequeue_timeout)
ERROR 06-24 23:58:53 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-24 23:58:53 [core.py:517]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 207, in get_response
ERROR 06-24 23:58:53 [core.py:517]     raise RuntimeError(
ERROR 06-24 23:58:53 [core.py:517] RuntimeError: Worker failed with error 'CUDA error: uncorrectable NVLink error detected during the execution
ERROR 06-24 23:58:53 [core.py:517] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
ERROR 06-24 23:58:53 [core.py:517] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
ERROR 06-24 23:58:53 [core.py:517] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
ERROR 06-24 23:58:53 [core.py:517] ', please check the stack trace above for the root cause
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]   File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1374, in execute_model
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] RuntimeError: CUDA error: uncorrectable NVLink error detected during the execution
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 
[1;36m(VllmWorker rank=2 pid=22352)[0;0m ERROR 06-24 23:58:53 [multiproc_executor.py:527] 

--- STDERR ---
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:00<00:05,  4.89it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:00<00:06,  4.30it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:00<00:06,  4.22it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:00<00:06,  4.27it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:01<00:05,  4.25it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:01<00:05,  4.28it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:01<00:04,  4.80it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:01<00:04,  4.70it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:02<00:04,  4.61it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:02<00:04,  4.52it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:02<00:04,  4.70it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:02<00:03,  4.87it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:02<00:03,  4.75it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:03<00:03,  4.62it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:03<00:03,  4.56it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:03<00:02,  4.75it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:03<00:02,  4.67it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:03<00:02,  4.60it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  63% Completed | 19/30 [00:04<00:02,  4.52it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  67% Completed | 20/30 [00:04<00:02,  4.49it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  70% Completed | 21/30 [00:04<00:02,  4.45it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  73% Completed | 22/30 [00:04<00:01,  4.65it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  80% Completed | 24/30 [00:05<00:01,  5.62it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  83% Completed | 25/30 [00:05<00:00,  5.27it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  87% Completed | 26/30 [00:05<00:00,  5.00it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  90% Completed | 27/30 [00:05<00:00,  4.81it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  93% Completed | 28/30 [00:05<00:00,  4.68it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards:  97% Completed | 29/30 [00:06<00:00,  4.64it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:06<00:00,  4.60it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:06<00:00,  4.66it/s]
[1;36m(VllmWorker rank=0 pid=22350)[0;0m 
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests:  31%|███       | 31/100 [00:00<00:00, 304.33it/s]Adding requests:  77%|███████▋  | 77/100 [00:00<00:00, 391.85it/s]Adding requests: 100%|██████████| 100/100 [00:00<00:00, 397.11it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank6]:[E624 23:58:53.614748939 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 6] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E624 23:58:53.614787179 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E624 23:58:53.614787029 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::DistBackendErrorc10::DistBackendError'
'
Traceback (most recent call last):
  File "/home/pirillo_google_com/vllm_benchmark/vllm-repo-benchmark/benchmarks/benchmark_throughput.py", line 724, in <module>
  what():  [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)
  what():  
[PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)
  what():  
[PG ID 2 PG GUID 3 Rank 6] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

    main(args)
  File "/home/pirillo_google_com/vllm_benchmark/vllm-repo-benchmark/benchmarks/benchmark_throughput.py", line 407, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/home/pirillo_google_com/vllm_benchmark/vllm-repo-benchmark/benchmarks/benchmark_throughput.py", line 92, in run_vllm
    outputs = llm.generate(
              ^^^^^^^^^^^^^
  File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/utils.py", line 1267, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
[rank5]:[E624 23:58:53.615783416 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 5] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

  File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 474, in generate
terminate called after throwing an instance of 'c10::DistBackendError'
    outputs = self._run_engine(use_tqdm=use_tqdm)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1517, in _run_engine
    step_outputs = self.llm_engine.step()
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 232, in step
    outputs = self.engine_core.get_output()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pirillo_google_com/.vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 625, in get_output
    raise self._format_exception(outputs) from None
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
  what():  [PG ID 2 PG GUID 3 Rank 5] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

[rank7]:[E624 23:58:53.617989127 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 7] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E624 23:58:53.618014677 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 4] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
[rank2]:[E624 23:58:53.618062344 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank0]:[E624 23:58:53.618138449 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 7] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 2 PG GUID 3 Rank 4] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: uncorrectable NVLink error detected during the execution
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x71c63b70d4a2 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x71c63bb42422 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x71c5caee5456 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x71c5caef56f0 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x71c5caef7282 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c5caef8e8d in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #8: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x71c63b7785e8 in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xcc7a4e (0x71c5caec7a4e in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9165ed (0x71c5cab165ed in /home/pirillo_google_com/.vllm/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x71c5baeecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x71c63c69caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c3c (0x71c63c729c3c in /lib/x86_64-linux-gnu/libc.so.6)

