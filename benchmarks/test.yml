# ==============================================================================
# VLLM Automated Benchmark Configuration (Single Server, Multi-Client)
# ==============================================================================
standalone_runs:
  - name: "Llama-3-3-70B"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [8]
    length_configs:
      - { input_len: 128, output_len: 1000 }
      - { input_len: 256, output_len: 256 }
      - { input_len: 1000, output_len: 1000 }
      - { input_len: 5000, output_len: 5000 }
    args: >
      --model "meta-llama/Llama-3.3-70B-Instruct"
      --num-prompts 10
      --gpu-memory-utilization 0.9
