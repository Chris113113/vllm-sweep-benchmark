# ==============================================================================
# Stage 1: Build Stage
#
# This stage starts from the NVIDIA CUDA development image to ensure all
# build tools and libraries (like nvcc) are present. It installs all Python 
# dependencies into a virtual environment.
# ==============================================================================
FROM vllm/vllm-openai:v0.9.1

# Set the working directory
WORKDIR /app

# Prevent Python from writing .pyc files
ENV PYTHONDONTWRITEBYTECODE 1
# Ensure Python output is sent straight to the terminal without buffering
ENV PYTHONUNBUFFERED 1

# Copy the requirements file into the container
COPY requirements.txt .

# Install the Python dependencies into the virtual environment
# This will use the pip from the venv. We use --no-cache-dir to reduce image size.
RUN pip install --no-cache-dir -r requirements.txt

# Copy all the local scripts into the container
COPY . .

# Ensure the shell script is executable
RUN chmod +x ./start_server.sh

# The entrypoint for the container. When the container runs, it will execute
# our main orchestration script. The command line arguments passed to
# `docker run` will be appended to this command.
ENTRYPOINT ["python3", "run_automated_benchmark.py"]

# Default command if none is provided to `docker run`.
# This shows the user how to run the script.
CMD ["--help"]
