[
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 80.2,
            "throughput_total_tokens_per_sec": 40986.28,
            "throughput_output_tokens_per_sec": 20531.72,
            "total_prompt_tokens": 127519,
            "total_output_tokens": 128000
        },
        "client_log_file": "1_Llama-3.1-8B-Instruct_TP-1_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 94.27,
            "throughput_total_tokens_per_sec": 48231.62,
            "throughput_output_tokens_per_sec": 24132.5,
            "total_prompt_tokens": 127823,
            "total_output_tokens": 128000
        },
        "client_log_file": "2_Llama-3.1-8B-Instruct_TP-2_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {
            "throughput_req_per_sec": 12.03,
            "throughput_total_tokens_per_sec": 24634.46,
            "throughput_output_tokens_per_sec": 12322.19,
            "total_prompt_tokens": 511588,
            "total_output_tokens": 512000
        },
        "client_log_file": "3_Llama-3.1-8B-Instruct_TP-1_ISL-1024_OSL-1024_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {
            "throughput_req_per_sec": 16.66,
            "throughput_total_tokens_per_sec": 34098.63,
            "throughput_output_tokens_per_sec": 17058.8,
            "total_prompt_tokens": 511431,
            "total_output_tokens": 512000
        },
        "client_log_file": "4_Llama-3.1-8B-Instruct_TP-2_ISL-1024_OSL-1024_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-1_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {
            "throughput_req_per_sec": 0.93,
            "throughput_total_tokens_per_sec": 7657.52,
            "throughput_output_tokens_per_sec": 3829.02,
            "total_prompt_tokens": 2047727,
            "total_output_tokens": 2048000
        },
        "client_log_file": "5_Llama-3.1-8B-Instruct_TP-1_ISL-4096_OSL-4096_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Llama-3.1-8B-Instruct_TP-2_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "num-prompts": "500",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {
            "throughput_req_per_sec": 1.63,
            "throughput_total_tokens_per_sec": 13377.09,
            "throughput_output_tokens_per_sec": 6690.07,
            "total_prompt_tokens": 2047066,
            "total_output_tokens": 2048000
        },
        "client_log_file": "6_Llama-3.1-8B-Instruct_TP-2_ISL-4096_OSL-4096_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 18.25,
            "throughput_total_tokens_per_sec": 9329.45,
            "throughput_output_tokens_per_sec": 4671.75,
            "total_prompt_tokens": 51046,
            "total_output_tokens": 51200
        },
        "client_log_file": "7_Gemma-3-27B-IT_TP-1_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 23.08,
            "throughput_total_tokens_per_sec": 11816.64,
            "throughput_output_tokens_per_sec": 5908.32,
            "total_prompt_tokens": 51200,
            "total_output_tokens": 51200
        },
        "client_log_file": "8_Gemma-3-27B-IT_TP-2_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-256_OSL-256",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "256",
            "output-len": "256",
            "max-model-len": "512"
        },
        "results": {
            "throughput_req_per_sec": 28.18,
            "throughput_total_tokens_per_sec": 14385.31,
            "throughput_output_tokens_per_sec": 7215.2,
            "total_prompt_tokens": 50880,
            "total_output_tokens": 51200
        },
        "client_log_file": "9_Gemma-3-27B-IT_TP-4_ISL-256_OSL-256_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {
            "throughput_req_per_sec": 2.39,
            "throughput_total_tokens_per_sec": 4903.83,
            "throughput_output_tokens_per_sec": 2451.91,
            "total_prompt_tokens": 204800,
            "total_output_tokens": 204800
        },
        "client_log_file": "10_Gemma-3-27B-IT_TP-1_ISL-1024_OSL-1024_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "11_Gemma-3-27B-IT_TP-2_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-1024_OSL-1024",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "1024",
            "output-len": "1024",
            "max-model-len": "2048"
        },
        "results": {},
        "client_log_file": "12_Gemma-3-27B-IT_TP-4_ISL-1024_OSL-1024_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-1_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "1",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "13_Gemma-3-27B-IT_TP-1_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-2_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "2",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {
            "throughput_req_per_sec": 0.83,
            "throughput_total_tokens_per_sec": 6791.13,
            "throughput_output_tokens_per_sec": 3397.18,
            "total_prompt_tokens": 818420,
            "total_output_tokens": 819200
        },
        "client_log_file": "14_Gemma-3-27B-IT_TP-2_ISL-4096_OSL-4096_vllm_script.log",
        "status": "SUCCESS"
    },
    {
        "run_name": "Gemma-3-27B-IT_TP-4_ISL-4096_OSL-4096",
        "mode": "vllm_throughput",
        "config": {
            "model": "google/gemma-3-27b-it",
            "num-prompts": "200",
            "gpu-memory-utilization": "0.9",
            "quantization": "fp8",
            "tensor-parallel-size": "4",
            "input-len": "4096",
            "output-len": "4096",
            "max-model-len": "8192"
        },
        "results": {},
        "client_log_file": "15_Gemma-3-27B-IT_TP-4_ISL-4096_OSL-4096_vllm_script.log",
        "status": "COMPLETED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-32B_TP-4_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Qwen3-4B_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-1024_OSL-1024",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-1_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-2_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-4-Maverick-17B-128E-Instruct_TP-4_ISL-4096_OSL-4096",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-2_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-4_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-8_ISL-256_OSL-256",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-2_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-4_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "DeepSeek-R1-671B_TP-8_ISL-1000_OSL-1000",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-256_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-256_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-1024_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-1024_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-1_ISL-5000_OSL-64",
        "status": "NOT_STARTED"
    },
    {
        "run_name": "Llama-Guard-4-12B_TP-2_ISL-5000_OSL-64",
        "status": "NOT_STARTED"
    }
]