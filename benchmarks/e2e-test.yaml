standalone_runs:
- name: "Llama-3.3-70b-Instruct"
  mode: "vllm_throughput"
  tensor_parallel_sizes: [1]
  args: >
    --model "meta-llama/Llama-3.3-70B-Instruct"
    --num-prompts 1000
    --gpu-memory-utilization 0.9
    --quantization fp8
    --dtype auto
  length_configs:
    - { input_len: 1024, output_len: 4096, prefix_len: 0 }