standalone_runs:
  # ==============================================================================
  # Llama-3.3-70B-Instruct
  # ==============================================================================
  
  - name: "Llama-3.3-70B-Instruct_CodeGen"
    mode: "vllm_throughput"
    tensor_parallel_sizes: [1,2,4,8]
    length_configs:
      - { input_len: 8192, output_len: 2048, prefix_len: 7900 }
    args: >
      --model "meta-llama/Llama-3.3-70B-Instruct"
      --num-prompts 50
      --gpu-memory-utilization 0.9
      --quantization fp8
      --dtype half

  # - name: "Llama-3.3-70B-Instruct_Summarization_2k"
  #   mode: "vllm_throughput"
  #   tensor_parallel_sizes: [2, 4, 8]
  #   length_configs:
  #     - { input_len: 2048, output_len: 256, prefix_len: 1800 }
  #   args: >
  #     --model "meta-llama/Llama-3.3-70B-Instruct"
  #     --num-prompts 200
  #     --gpu-memory-utilization 0.9
  #     --quantization fp8
  #     --dtype half

  # - name: "Llama-3.3-70B-Instruct_Summarization_4k"
  #   mode: "vllm_throughput"
  #   tensor_parallel_sizes: [2, 4, 8]
  #   length_configs:
  #     - { input_len: 4096, output_len: 256, prefix_len: 1800 }
  #   args: >
  #     --model "meta-llama/Llama-3.3-70B-Instruct"
  #     --num-prompts 200
  #     --gpu-memory-utilization 0.9
  #     --quantization fp8
  #     --dtype half
